{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishardmunene/Stable-Diffusion-test/blob/train/SDXL_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGxmTWQ7BqJA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union, List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import amp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from diffusers import StableDiffusionXLPipeline, ControlNetModel\n",
        "from accelerate import Accelerator\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from typing import Dict, Any, List, Optional, Union, Tuple\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import logging\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from typing import Dict, Any, Optional\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdW3U3ihCSNt",
        "outputId": "a3b819f5-0656-4b02-8778-0956ac02d566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 2. Mount drive and setup paths\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ROOT_DIR = '/content/drive/MyDrive/SDXL_images'\n",
        "IMAGE_DIR = os.path.join(ROOT_DIR, 'landscape_images')\n",
        "OUTPUT_DIR = os.path.join(ROOT_DIR, 'generated_images')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 3. Update ImageFolderDataset class\n",
        "class ImageFolderDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: Tuple[int, int] = (512, 512),\n",
        "        transform = None\n",
        "    ):\n",
        "        self.folder_path = Path(IMAGE_DIR)\n",
        "        self._validate_folder()\n",
        "\n",
        "        self.image_files = self._get_image_files()\n",
        "        self.image_size = image_size\n",
        "        self.transform = transform or self._default_transform()\n",
        "\n",
        "        print(f\"Loaded dataset with {len(self.image_files)} images from {IMAGE_DIR}\")\n",
        "\n",
        "    def _validate_folder(self):\n",
        "        if not self.folder_path.exists():\n",
        "            raise ValueError(f\"Image directory not found: {IMAGE_DIR}\")\n",
        "\n",
        "    def _get_image_files(self) -> List[Path]:\n",
        "        files = list(self.folder_path.glob(\"*.jpg\")) + \\\n",
        "                list(self.folder_path.glob(\"*.png\"))\n",
        "        if not files:\n",
        "            raise ValueError(f\"No images found in {IMAGE_DIR}\")\n",
        "        return files\n",
        "\n",
        "    def _default_transform(self):\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(self.image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_files[idx]\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return {\n",
        "                \"image\": image,\n",
        "                \"prompt\": f\"A landscape photo of {image_path.stem}\",\n",
        "                \"path\": str(image_path)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKjBY8vKBx88"
      },
      "outputs": [],
      "source": [
        "def monitor_memory():\n",
        "    \"\"\"Utility function to monitor memory usage.\"\"\"\n",
        "    gpu_memory = torch.cuda.memory_allocated() / 1024**2\n",
        "    ram_memory = psutil.Process().memory_info().rss / 1024**2\n",
        "    return {\n",
        "        \"gpu_memory_mb\": gpu_memory,\n",
        "        \"ram_memory_mb\": ram_memory\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZo1g8VWCSNv"
      },
      "outputs": [],
      "source": [
        "class PromptProcessor:\n",
        "    def __init__(self, model_name: str = \"bert-base-uncased\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def process_prompt(self, prompt: str) -> Dict[str, Any]:\n",
        "        parameters = self._extract_parameters(prompt)\n",
        "        return parameters\n",
        "\n",
        "    def _extract_parameters(self, prompt: str) -> Dict[str, Any]:\n",
        "        params = {}\n",
        "        if \"large\" in prompt.lower():\n",
        "            params[\"size\"] = (1024, 1024)\n",
        "        elif \"small\" in prompt.lower():\n",
        "            params[\"size\"] = (256, 256)\n",
        "        else:\n",
        "            params[\"size\"] = (512, 512)\n",
        "        return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGro1-oXB2gE"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class OptimizationConfig:\n",
        "    image_size: Tuple[int, int] = (256, 256)\n",
        "    precision: str = \"fp16\"  # or \"bf16\"\n",
        "    enable_checkpointing: bool = True\n",
        "    enable_attention_slicing: bool = True\n",
        "    enable_sequential_cpu_offload: bool = False\n",
        "    vae_slicing: bool = True\n",
        "\n",
        "class ProcessedPrompt:\n",
        "    raw_prompt: str\n",
        "    tokens: List[str]\n",
        "    parameters: Dict[str, Any]\n",
        "    intent: str\n",
        "    confidence: float\n",
        "\n",
        "class PromptProcessor:\n",
        "    def __init__(self, model_name: str = \"bert-base-uncased\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def process_prompt(self, prompt: str) -> ProcessedPrompt:\n",
        "        self.logger.info(f\"Processing prompt: {prompt}\")\n",
        "\n",
        "        # Tokenization\n",
        "        tokens = self.tokenizer.tokenize(prompt)\n",
        "\n",
        "        # Basic parameter extraction\n",
        "        parameters = self._extract_parameters(prompt)\n",
        "\n",
        "        # Simple intent classification\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(**inputs).logits\n",
        "            intent_id = torch.argmax(logits, dim=1).item()\n",
        "            confidence = torch.softmax(logits, dim=1)[0][intent_id].item()\n",
        "\n",
        "        return ProcessedPrompt(\n",
        "            raw_prompt=prompt,\n",
        "            tokens=tokens,\n",
        "            parameters=parameters,\n",
        "            intent=str(intent_id),\n",
        "            confidence=confidence\n",
        "        )\n",
        "\n",
        "    def _extract_parameters(self, prompt: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract parameters from prompt\"\"\"\n",
        "        # Basic parameter extraction\n",
        "        params = {}\n",
        "        # Add size detection\n",
        "        if \"large\" in prompt.lower():\n",
        "            params[\"size\"] = (1024, 1024)\n",
        "        elif \"small\" in prompt.lower():\n",
        "            params[\"size\"] = (256, 256)\n",
        "        else:\n",
        "            params[\"size\"] = (512, 512)\n",
        "\n",
        "        return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz35mwanB611"
      },
      "outputs": [],
      "source": [
        "class OptimizedSDXL:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str = \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        config: Optional[OptimizationConfig] = None\n",
        "    ):\n",
        "        self.config = config or OptimizationConfig()\n",
        "        self.accelerator = Accelerator()\n",
        "        self.setup_pipeline(model_id)\n",
        "        self.optimizer = None\n",
        "        self.loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    def setup_pipeline(self, model_id: str):\n",
        "        \"\"\"Setup and optimize the SDXL pipeline\"\"\"\n",
        "        try:\n",
        "            # Initialize pipeline with optimizations\n",
        "            self.pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=torch.float16 if self.config.precision == \"fp16\" else torch.bfloat16,\n",
        "                use_safetensors=True\n",
        "            )\n",
        "\n",
        "            # Apply memory optimizations\n",
        "            if self.config.enable_attention_slicing:\n",
        "                self.pipeline.enable_attention_slicing()\n",
        "\n",
        "            if self.config.enable_sequential_cpu_offload:\n",
        "                self.pipeline.enable_sequential_cpu_offload()\n",
        "\n",
        "            if self.config.vae_slicing:\n",
        "                self.pipeline.enable_vae_slicing()\n",
        "\n",
        "            # Move to accelerator device\n",
        "            self.pipeline = self.pipeline.to(self.accelerator.device)\n",
        "\n",
        "            # Enable gradient checkpointing if configured\n",
        "            if self.config.enable_checkpointing:\n",
        "                self.pipeline.unet.enable_gradient_checkpointing()\n",
        "\n",
        "            print(f\"Pipeline setup complete on device: {self.accelerator.device}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate_image(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: Optional[str] = None,\n",
        "        num_inference_steps: int = 50,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Generate image with the pipeline\"\"\"\n",
        "        try:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                result = self.pipeline(\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    **kwargs\n",
        "                ).images[0]\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating image: {str(e)}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6MalIiCCBH4"
      },
      "outputs": [],
      "source": [
        "class ProgressiveTrainer:\n",
        "    def __init__(self, sdxl_model: OptimizedSDXL):\n",
        "        self.model = sdxl_model\n",
        "        self.progressive_sizes = [(256, 256), (512, 512), (768, 768), (1024, 1024)]\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def train_progressive(self, dataset, num_epochs_per_size=5):\n",
        "        \"\"\"Implement progressive training with increasing image sizes.\"\"\"\n",
        "        for size in self.progressive_sizes:\n",
        "            try:\n",
        "                print(f\"Starting training at resolution {size}\")\n",
        "                # Update model config and resize dataset images if needed\n",
        "                self.model.config.image_size = size\n",
        "                resized_dataset = self.resize_dataset(dataset, size)\n",
        "                self.train_single_stage(resized_dataset, num_epochs_per_size)\n",
        "                print(f\"Completed training at resolution {size}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during training at resolution {size}: {e}\")\n",
        "                continue\n",
        "\n",
        "    def resize_dataset(self, dataset, size):\n",
        "        \"\"\"Resize dataset images to target size.\"\"\"\n",
        "        # Implement dataset resizing logic here\n",
        "        # Placeholder implementation; modify as per dataset structure\n",
        "        return dataset\n",
        "\n",
        "    def train_single_stage(self, dataset, num_epochs):\n",
        "        \"\"\"Training loop for a single resolution stage.\"\"\"\n",
        "        try:\n",
        "            # Ensure the model and subcomponents are in training mode\n",
        "            if hasattr(self.model, 'train'):\n",
        "                self.model.train()\n",
        "            else:\n",
        "                self.model.pipeline.train()\n",
        "\n",
        "            # Move the model to the appropriate device\n",
        "            self.model.pipeline.to(self.device)\n",
        "\n",
        "            # Enable gradient checkpointing if supported\n",
        "            if hasattr(self.model, 'apply_gradient_checkpointing'):\n",
        "                self.model.apply_gradient_checkpointing()\n",
        "\n",
        "            scaler = torch.cuda.amp.GradScaler()\n",
        "            optimizer = torch.optim.AdamW(self.model.pipeline.unet.parameters(), lr=1e-5)\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                total_loss = 0\n",
        "                num_batches = 0\n",
        "\n",
        "                for batch in dataset:\n",
        "                    try:\n",
        "                        # Move batch to the correct device\n",
        "                        batch = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
        "\n",
        "                        # Clear gradients\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                        # Forward pass with automatic mixed precision\n",
        "                        with torch.cuda.amp.autocast():\n",
        "                            loss = self.model.pipeline(batch)  # Forward pass\n",
        "                            total_loss += loss.item()\n",
        "\n",
        "                        # Backward pass with gradient scaling\n",
        "                        scaler.scale(loss).backward()\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "\n",
        "                        num_batches += 1\n",
        "                    except Exception as batch_error:\n",
        "                        print(f\"Error during batch processing: {batch_error}\")\n",
        "                        continue\n",
        "\n",
        "                # Print epoch statistics\n",
        "                avg_loss = total_loss / max(num_batches, 1)  # Avoid division by zero\n",
        "                print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                # Clear CUDA cache and garbage collect after each epoch\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during training stage: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8WVXaz4CFz_"
      },
      "outputs": [],
      "source": [
        "def upload_images():\n",
        "    \"\"\"Allow users to upload images for training.\"\"\"\n",
        "    os.makedirs(\"uploaded_images\", exist_ok=True)\n",
        "    uploaded_files = files.upload()\n",
        "    for filename in uploaded_files.keys():\n",
        "        img = Image.open(filename)\n",
        "        img.save(f\"uploaded_images/{filename}\")\n",
        "        print(f\"Saved {filename} to uploaded_images/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDlLJVS6CJWo"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "def prepare_dataset():\n",
        "    \"\"\"Prepare dataset from uploaded images.\"\"\"\n",
        "    image_transforms = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    return ImageDataset(\"uploaded_images\", transform=image_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_landscape_image(\n",
        "    model: OptimizedSDXL,\n",
        "    prompt: str,\n",
        "    processor: Optional[PromptProcessor] = None,\n",
        "    negative_prompt: Optional[str] = None,\n",
        "    num_inference_steps: int = 50,\n",
        "    max_attempts: int = 1\n",
        ") -> Optional[Image.Image]:\n",
        "    \"\"\"Generate landscape image with attempt limiting\"\"\"\n",
        "\n",
        "    attempt = 0\n",
        "    while attempt < max_attempts:\n",
        "        try:\n",
        "            print(f\"Attempt {attempt + 1}/{max_attempts} - Generating image...\")\n",
        "\n",
        "            # Process prompt if processor available\n",
        "            parameters = {}\n",
        "            if processor:\n",
        "                parameters = processor.process_prompt(prompt)\n",
        "\n",
        "            # Clear memory\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Generate image\n",
        "            image = model.generate_image(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                **parameters\n",
        "            )\n",
        "\n",
        "            if image:\n",
        "                print(f\"Successfully generated image on attempt {attempt + 1}\")\n",
        "                return image\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error on attempt {attempt + 1}: {str(e)}\")\n",
        "\n",
        "        attempt += 1\n",
        "\n",
        "    print(\"Failed to generate image after maximum attempts\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "swYmgaykHnZS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "f9a8d5df76a845e6882ef4dc35b33fa0",
            "96cf39f07999467489cda453b8195432",
            "badd76bd3488471f808c8edf1387b5f7",
            "002f5f8880394d08ac9c8d519c8d2586",
            "187b14a27cfb403f96e4173a6743699a",
            "3eea52b617ca499db6f5f4651b7e037d",
            "337c55c5740542a0ac6ca8c1dc2a60db",
            "050e865ce90c4a4f9b2543c60122e5fe",
            "9823b613e954464fb656eed75cf79d1e",
            "9cefd54e9b974f0c83d470c216d03a52",
            "5a8275422b824664ba668ca118607bf7"
          ]
        },
        "id": "tLKkiKRNCSNz",
        "outputId": "d37ac1a4-efb6-4e96-9e73-856c3d8d0a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 7 images from /content/drive/MyDrive/SDXL_images/landscape_images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9a8d5df76a845e6882ef4dc35b33fa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline setup complete on device: cuda\n",
            "Attempt 1/1 - Generating image...\n",
            "Error on attempt 1: ProcessedPrompt() takes no arguments\n",
            "Failed to generate image after maximum attempts\n"
          ]
        }
      ],
      "source": [
        "# Add this in a new cell after your class definitions:\n",
        "\n",
        "# Initialize components\n",
        "dataset = ImageFolderDataset(image_size=(512, 512))\n",
        "processor = PromptProcessor()\n",
        "config = OptimizationConfig(image_size=(512, 512))\n",
        "model = OptimizedSDXL(config=config)\n",
        "\n",
        "# Generate image using dataset\n",
        "if len(dataset) > 0:\n",
        "    sample = dataset[0]  # Get first image\n",
        "    prompt = sample[\"prompt\"]\n",
        "\n",
        "    image = generate_landscape_image(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        processor=processor\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uSDvg407COtJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "958b46e3-a224-465a-ad9d-c12c7faa76ad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DriveImageDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4ee3ce27a3c7>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m def train_model(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizedSDXL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDriveImageDataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DriveImageDataset' is not defined"
          ]
        }
      ],
      "source": [
        "def train_model(\n",
        "    model: OptimizedSDXL,\n",
        "    dataset: DriveImageDataset,\n",
        "    num_epochs: int = 5,\n",
        "    batch_size: int = 1,\n",
        "    save_interval: int = 100\n",
        "):\n",
        "    \"\"\"Train model using Drive dataset\"\"\"\n",
        "    try:\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "            for batch_idx, batch in enumerate(dataloader):\n",
        "                # Process batch\n",
        "                images = batch[\"image\"]\n",
        "                prompts = batch[\"prompt\"]\n",
        "\n",
        "                # Generate images\n",
        "                generated = model.generate_image(\n",
        "                    prompt=prompts[0],  # Using first prompt\n",
        "                    num_inference_steps=50\n",
        "                )\n",
        "\n",
        "                # Save checkpoint to Drive\n",
        "                if batch_idx % save_interval == 0:\n",
        "                    checkpoint_path = os.path.join(\n",
        "                        CHECKPOINT_DIR,\n",
        "                        f\"checkpoint_e{epoch}_b{batch_idx}.pt\"\n",
        "                    )\n",
        "                    torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "                # Clear memory\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czxItnMLCTZ3"
      },
      "outputs": [],
      "source": [
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl4RCy69ZRWH"
      },
      "outputs": [],
      "source": [
        "def generate_landscape_image(\n",
        "    model: OptimizedSDXL,\n",
        "    prompt: str,\n",
        "    processor: Optional[PromptProcessor] = None,\n",
        "    negative_prompt: Optional[str] = None,\n",
        "    num_inference_steps: int = 50\n",
        ") -> Optional[Image.Image]:\n",
        "    \"\"\"\n",
        "    Generate landscape image with prompt processing and memory optimization.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Process prompt if processor provided\n",
        "        if processor:\n",
        "            processed = processor.process_prompt(prompt)\n",
        "            parameters = processed.parameters\n",
        "        else:\n",
        "            parameters = {}\n",
        "\n",
        "        print(f\"Generating landscape image for prompt: '{prompt}'...\")\n",
        "\n",
        "        # Clear CUDA cache\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Generate image with processed parameters\n",
        "        image = model.generate_image(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            **parameters\n",
        "        )\n",
        "\n",
        "        # Save image\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        save_path = f\"landscape_image_{timestamp}.png\"\n",
        "        image.save(save_path)\n",
        "        print(f\"Landscape image saved at {save_path}\")\n",
        "\n",
        "        return image\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWiXkzENh7p2"
      },
      "outputs": [],
      "source": [
        "# Initialize model and generate image\n",
        "config = OptimizationConfig(image_size=(256, 256))\n",
        "optimized_sdxl = OptimizedSDXL(config=config)\n",
        "\n",
        "# Generate an image\n",
        "landscape_prompt = \"A picturesque mountain range with a tranquil river flowing through it\"\n",
        "generate_landscape_image(optimized_sdxl, prompt=landscape_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hZUfE3GVE2F"
      },
      "outputs": [],
      "source": [
        "# 1. First, fix the generate_landscape_image function definition\n",
        "def generate_landscape_image(\n",
        "    model: OptimizedSDXL,\n",
        "    prompt: str,\n",
        "    negative_prompt: Optional[str] = None,\n",
        "    num_inference_steps: int = 50,\n",
        "    processor: Optional[PromptProcessor] = None  # Add processor parameter\n",
        ") -> Optional[Image.Image]:\n",
        "    \"\"\"Generate landscape image with prompt processing and memory optimization.\"\"\"\n",
        "    try:\n",
        "        # Process prompt if processor provided\n",
        "        parameters = {}\n",
        "        if processor:\n",
        "            parameters = processor.process_prompt(prompt)\n",
        "\n",
        "        print(f\"Generating image for prompt: '{prompt}'...\")\n",
        "\n",
        "        # Clear memory\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Generate with parameters\n",
        "        image = model.generate_image(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            **parameters\n",
        "        )\n",
        "\n",
        "        return image\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2DUYxIoVE2G"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize processor and model\n",
        "    processor = PromptProcessor()\n",
        "    config = OptimizationConfig(image_size=(512, 512))\n",
        "    model = OptimizedSDXL(config=config)\n",
        "\n",
        "    # Generate image with processed prompt\n",
        "    prompt = \"A large mountain landscape with dramatic sunset lighting\"\n",
        "    image = generate_landscape_image(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        negative_prompt=\"blur, distortion, low quality\",\n",
        "        num_inference_steps=50,\n",
        "        processor=processor  # Pass processor as a named argument\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9a8d5df76a845e6882ef4dc35b33fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96cf39f07999467489cda453b8195432",
              "IPY_MODEL_badd76bd3488471f808c8edf1387b5f7",
              "IPY_MODEL_002f5f8880394d08ac9c8d519c8d2586"
            ],
            "layout": "IPY_MODEL_187b14a27cfb403f96e4173a6743699a"
          }
        },
        "96cf39f07999467489cda453b8195432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eea52b617ca499db6f5f4651b7e037d",
            "placeholder": "​",
            "style": "IPY_MODEL_337c55c5740542a0ac6ca8c1dc2a60db",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "badd76bd3488471f808c8edf1387b5f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_050e865ce90c4a4f9b2543c60122e5fe",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9823b613e954464fb656eed75cf79d1e",
            "value": 7
          }
        },
        "002f5f8880394d08ac9c8d519c8d2586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cefd54e9b974f0c83d470c216d03a52",
            "placeholder": "​",
            "style": "IPY_MODEL_5a8275422b824664ba668ca118607bf7",
            "value": " 7/7 [01:50&lt;00:00, 14.24s/it]"
          }
        },
        "187b14a27cfb403f96e4173a6743699a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eea52b617ca499db6f5f4651b7e037d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "337c55c5740542a0ac6ca8c1dc2a60db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "050e865ce90c4a4f9b2543c60122e5fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9823b613e954464fb656eed75cf79d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cefd54e9b974f0c83d470c216d03a52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a8275422b824664ba668ca118607bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}