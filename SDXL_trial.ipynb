{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishardmunene/Stable-Diffusion-test/blob/train/SDXL_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "09b815fe79a346068c7430a2f6f77657",
            "53aa0965fdb6414e9085c5d4b84c2b9f",
            "c6719c8309d149f6909ae09ef1369868",
            "893d98e8b2264fe287bf4da582897c0a",
            "249aca2ae7c94eb687e2e31e17219f57",
            "c11832035b7d41149754f0c50cfb7fdd",
            "dbc488b7e34445cca196937d7234e27a",
            "597974790f1e40c381046005e499f80b",
            "1b41051e04e244119a79399022c1bc03",
            "b4fc1c390fe141b68c89c3748adda8f3",
            "a578cb57bab247c3acf77851ec28e866"
          ]
        },
        "id": "sGxmTWQ7BqJA",
        "outputId": "ca2f8462-3a18-4603-fa6f-8631367bade0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09b815fe79a346068c7430a2f6f77657",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union, List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import amp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from diffusers import StableDiffusionXLPipeline, ControlNetModel\n",
        "from accelerate import Accelerator\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from typing import Dict, Any, List, Optional, Union, Tuple\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import logging\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from typing import Dict, Any, Optional\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdW3U3ihCSNt"
      },
      "outputs": [],
      "source": [
        "# 2. Mount drive and setup paths\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ROOT_DIR = '/content/drive/MyDrive/SDXL_images'\n",
        "IMAGE_DIR = os.path.join(ROOT_DIR, 'landscape_images')\n",
        "OUTPUT_DIR = os.path.join(ROOT_DIR, 'generated_images')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 3. Update ImageFolderDataset class\n",
        "class ImageFolderDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: Tuple[int, int] = (512, 512),\n",
        "        transform = None\n",
        "    ):\n",
        "        self.folder_path = Path(IMAGE_DIR)\n",
        "        self._validate_folder()\n",
        "\n",
        "        self.image_files = self._get_image_files()\n",
        "        self.image_size = image_size\n",
        "        self.transform = transform or self._default_transform()\n",
        "\n",
        "        print(f\"Loaded dataset with {len(self.image_files)} images from {IMAGE_DIR}\")\n",
        "\n",
        "    def _validate_folder(self):\n",
        "        if not self.folder_path.exists():\n",
        "            raise ValueError(f\"Image directory not found: {IMAGE_DIR}\")\n",
        "\n",
        "    def _get_image_files(self) -> List[Path]:\n",
        "        files = list(self.folder_path.glob(\"*.jpg\")) + \\\n",
        "                list(self.folder_path.glob(\"*.png\"))\n",
        "        if not files:\n",
        "            raise ValueError(f\"No images found in {IMAGE_DIR}\")\n",
        "        return files\n",
        "\n",
        "    def _default_transform(self):\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(self.image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_files[idx]\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return {\n",
        "                \"image\": image,\n",
        "                \"prompt\": f\"A landscape photo of {image_path.stem}\",\n",
        "                \"path\": str(image_path)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKjBY8vKBx88"
      },
      "outputs": [],
      "source": [
        "def monitor_memory():\n",
        "    \"\"\"Utility function to monitor memory usage.\"\"\"\n",
        "    gpu_memory = torch.cuda.memory_allocated() / 1024**2\n",
        "    ram_memory = psutil.Process().memory_info().rss / 1024**2\n",
        "    return {\n",
        "        \"gpu_memory_mb\": gpu_memory,\n",
        "        \"ram_memory_mb\": ram_memory\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZo1g8VWCSNv"
      },
      "outputs": [],
      "source": [
        "class PromptProcessor:\n",
        "    def __init__(self, model_name: str = \"bert-base-uncased\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def process_prompt(self, prompt: str) -> Dict[str, Any]:\n",
        "        parameters = self._extract_parameters(prompt)\n",
        "        return parameters\n",
        "\n",
        "    def _extract_parameters(self, prompt: str) -> Dict[str, Any]:\n",
        "        params = {}\n",
        "        if \"large\" in prompt.lower():\n",
        "            params[\"size\"] = (1024, 1024)\n",
        "        elif \"small\" in prompt.lower():\n",
        "            params[\"size\"] = (256, 256)\n",
        "        else:\n",
        "            params[\"size\"] = (512, 512)\n",
        "        return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGro1-oXB2gE"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class OptimizationConfig:\n",
        "    image_size: Tuple[int, int] = (256, 256)\n",
        "    precision: str = \"fp16\"  # or \"bf16\"\n",
        "    enable_checkpointing: bool = True\n",
        "    enable_attention_slicing: bool = True\n",
        "    enable_sequential_cpu_offload: bool = False\n",
        "    vae_slicing: bool = True\n",
        "\n",
        "class ProcessedPrompt:\n",
        "    raw_prompt: str\n",
        "    tokens: List[str]\n",
        "    parameters: Dict[str, Any]\n",
        "    intent: str\n",
        "    confidence: float\n",
        "\n",
        "class PromptProcessor:\n",
        "    def __init__(self, model_name: str = \"bert-base-uncased\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def process_prompt(self, prompt: str) -> ProcessedPrompt:\n",
        "        self.logger.info(f\"Processing prompt: {prompt}\")\n",
        "\n",
        "        # Tokenization\n",
        "        tokens = self.tokenizer.tokenize(prompt)\n",
        "\n",
        "        # Basic parameter extraction\n",
        "        parameters = self._extract_parameters(prompt)\n",
        "\n",
        "        # Simple intent classification\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(**inputs).logits\n",
        "            intent_id = torch.argmax(logits, dim=1).item()\n",
        "            confidence = torch.softmax(logits, dim=1)[0][intent_id].item()\n",
        "\n",
        "        return ProcessedPrompt(\n",
        "            raw_prompt=prompt,\n",
        "            tokens=tokens,\n",
        "            parameters=parameters,\n",
        "            intent=str(intent_id),\n",
        "            confidence=confidence\n",
        "        )\n",
        "\n",
        "    def _extract_parameters(self, prompt: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract parameters from prompt\"\"\"\n",
        "        # Basic parameter extraction\n",
        "        params = {}\n",
        "        # Add size detection\n",
        "        if \"large\" in prompt.lower():\n",
        "            params[\"size\"] = (1024, 1024)\n",
        "        elif \"small\" in prompt.lower():\n",
        "            params[\"size\"] = (256, 256)\n",
        "        else:\n",
        "            params[\"size\"] = (512, 512)\n",
        "\n",
        "        return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz35mwanB611"
      },
      "outputs": [],
      "source": [
        "class OptimizedSDXL:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str = \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        config: Optional[OptimizationConfig] = None\n",
        "    ):\n",
        "        self.config = config or OptimizationConfig()\n",
        "        self.accelerator = Accelerator()\n",
        "        self.setup_pipeline(model_id)\n",
        "        self.optimizer = None\n",
        "        self.loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    def setup_pipeline(self, model_id: str):\n",
        "        \"\"\"Setup and optimize the SDXL pipeline\"\"\"\n",
        "        try:\n",
        "            # Initialize pipeline with optimizations\n",
        "            self.pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=torch.float16 if self.config.precision == \"fp16\" else torch.bfloat16,\n",
        "                use_safetensors=True\n",
        "            )\n",
        "\n",
        "            # Apply memory optimizations\n",
        "            if self.config.enable_attention_slicing:\n",
        "                self.pipeline.enable_attention_slicing()\n",
        "\n",
        "            if self.config.enable_sequential_cpu_offload:\n",
        "                self.pipeline.enable_sequential_cpu_offload()\n",
        "\n",
        "            if self.config.vae_slicing:\n",
        "                self.pipeline.enable_vae_slicing()\n",
        "\n",
        "            # Move to accelerator device\n",
        "            self.pipeline = self.pipeline.to(self.accelerator.device)\n",
        "\n",
        "            # Enable gradient checkpointing if configured\n",
        "            if self.config.enable_checkpointing:\n",
        "                self.pipeline.unet.enable_gradient_checkpointing()\n",
        "\n",
        "            print(f\"Pipeline setup complete on device: {self.accelerator.device}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate_image(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: Optional[str] = None,\n",
        "        num_inference_steps: int = 50,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Generate image with the pipeline\"\"\"\n",
        "        try:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                result = self.pipeline(\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    **kwargs\n",
        "                ).images[0]\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating image: {str(e)}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6MalIiCCBH4"
      },
      "outputs": [],
      "source": [
        "class ProgressiveTrainer:\n",
        "    def __init__(self, sdxl_model: OptimizedSDXL):\n",
        "        self.model = sdxl_model\n",
        "        self.progressive_sizes = [(256, 256), (512, 512), (768, 768), (1024, 1024)]\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def train_progressive(self, dataset, num_epochs_per_size=5):\n",
        "        \"\"\"Implement progressive training with increasing image sizes.\"\"\"\n",
        "        for size in self.progressive_sizes:\n",
        "            try:\n",
        "                print(f\"Starting training at resolution {size}\")\n",
        "                # Update model config and resize dataset images if needed\n",
        "                self.model.config.image_size = size\n",
        "                resized_dataset = self.resize_dataset(dataset, size)\n",
        "                self.train_single_stage(resized_dataset, num_epochs_per_size)\n",
        "                print(f\"Completed training at resolution {size}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during training at resolution {size}: {e}\")\n",
        "                continue\n",
        "\n",
        "    def resize_dataset(self, dataset, size):\n",
        "        \"\"\"Resize dataset images to target size.\"\"\"\n",
        "        # Implement dataset resizing logic here\n",
        "        # Placeholder implementation; modify as per dataset structure\n",
        "        return dataset\n",
        "\n",
        "    def train_single_stage(self, dataset, num_epochs):\n",
        "        \"\"\"Training loop for a single resolution stage.\"\"\"\n",
        "        try:\n",
        "            # Ensure the model and subcomponents are in training mode\n",
        "            if hasattr(self.model, 'train'):\n",
        "                self.model.train()\n",
        "            else:\n",
        "                self.model.pipeline.train()\n",
        "\n",
        "            # Move the model to the appropriate device\n",
        "            self.model.pipeline.to(self.device)\n",
        "\n",
        "            # Enable gradient checkpointing if supported\n",
        "            if hasattr(self.model, 'apply_gradient_checkpointing'):\n",
        "                self.model.apply_gradient_checkpointing()\n",
        "\n",
        "            scaler = torch.cuda.amp.GradScaler()\n",
        "            optimizer = torch.optim.AdamW(self.model.pipeline.unet.parameters(), lr=1e-5)\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                total_loss = 0\n",
        "                num_batches = 0\n",
        "\n",
        "                for batch in dataset:\n",
        "                    try:\n",
        "                        # Move batch to the correct device\n",
        "                        batch = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
        "\n",
        "                        # Clear gradients\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                        # Forward pass with automatic mixed precision\n",
        "                        with torch.cuda.amp.autocast():\n",
        "                            loss = self.model.pipeline(batch)  # Forward pass\n",
        "                            total_loss += loss.item()\n",
        "\n",
        "                        # Backward pass with gradient scaling\n",
        "                        scaler.scale(loss).backward()\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "\n",
        "                        num_batches += 1\n",
        "                    except Exception as batch_error:\n",
        "                        print(f\"Error during batch processing: {batch_error}\")\n",
        "                        continue\n",
        "\n",
        "                # Print epoch statistics\n",
        "                avg_loss = total_loss / max(num_batches, 1)  # Avoid division by zero\n",
        "                print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                # Clear CUDA cache and garbage collect after each epoch\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during training stage: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8WVXaz4CFz_"
      },
      "outputs": [],
      "source": [
        "def upload_images():\n",
        "    \"\"\"Allow users to upload images for training.\"\"\"\n",
        "    os.makedirs(\"uploaded_images\", exist_ok=True)\n",
        "    uploaded_files = files.upload()\n",
        "    for filename in uploaded_files.keys():\n",
        "        img = Image.open(filename)\n",
        "        img.save(f\"uploaded_images/{filename}\")\n",
        "        print(f\"Saved {filename} to uploaded_images/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDlLJVS6CJWo"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "def prepare_dataset():\n",
        "    \"\"\"Prepare dataset from uploaded images.\"\"\"\n",
        "    image_transforms = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    return ImageDataset(\"uploaded_images\", transform=image_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLKkiKRNCSNz"
      },
      "outputs": [],
      "source": [
        "# Add this in a new cell after your class definitions:\n",
        "\n",
        "# Initialize components\n",
        "dataset = ImageFolderDataset(image_size=(512, 512))\n",
        "processor = PromptProcessor()\n",
        "config = OptimizationConfig(image_size=(512, 512))\n",
        "model = OptimizedSDXL(config=config)\n",
        "\n",
        "# Generate image using dataset\n",
        "if len(dataset) > 0:\n",
        "    sample = dataset[0]  # Get first image\n",
        "    prompt = sample[\"prompt\"]\n",
        "\n",
        "    image = generate_landscape_image(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        processor=processor\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSDvg407COtJ"
      },
      "outputs": [],
      "source": [
        "def train_model():\n",
        "    \"\"\"Train the SDXL model progressively with uploaded images.\"\"\"\n",
        "    # Upload images\n",
        "    upload_images()\n",
        "\n",
        "    # Prepare dataset\n",
        "    dataset = prepare_dataset()\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "    # Initialize OptimizedSDXL\n",
        "    config = OptimizationConfig(image_size=(256, 256))  # Start at 256x256 resolution\n",
        "    optimized_sdxl = OptimizedSDXL(config=config)\n",
        "\n",
        "    # Initialize ProgressiveTrainer\n",
        "    trainer = ProgressiveTrainer(sdxl_model=optimized_sdxl)\n",
        "\n",
        "    # Train model\n",
        "    trainer.train_progressive(dataset=dataloader, num_epochs_per_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "czxItnMLCTZ3",
        "outputId": "ec5f75cd-1792-48e7-f684-b2ace321f2b8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9cec8d64-de1a-4859-9193-7a741cfae107\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9cec8d64-de1a-4859-9193-7a741cfae107\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl4RCy69ZRWH"
      },
      "outputs": [],
      "source": [
        "def generate_landscape_image(\n",
        "    model: OptimizedSDXL,\n",
        "    prompt: str,\n",
        "    processor: Optional[PromptProcessor] = None,\n",
        "    negative_prompt: Optional[str] = None,\n",
        "    num_inference_steps: int = 50\n",
        ") -> Optional[Image.Image]:\n",
        "    \"\"\"\n",
        "    Generate landscape image with prompt processing and memory optimization.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Process prompt if processor provided\n",
        "        if processor:\n",
        "            processed = processor.process_prompt(prompt)\n",
        "            parameters = processed.parameters\n",
        "        else:\n",
        "            parameters = {}\n",
        "\n",
        "        print(f\"Generating landscape image for prompt: '{prompt}'...\")\n",
        "\n",
        "        # Clear CUDA cache\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Generate image with processed parameters\n",
        "        image = model.generate_image(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            **parameters\n",
        "        )\n",
        "\n",
        "        # Save image\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        save_path = f\"landscape_image_{timestamp}.png\"\n",
        "        image.save(save_path)\n",
        "        print(f\"Landscape image saved at {save_path}\")\n",
        "\n",
        "        return image\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWiXkzENh7p2"
      },
      "outputs": [],
      "source": [
        "# Initialize model and generate image\n",
        "config = OptimizationConfig(image_size=(256, 256))\n",
        "optimized_sdxl = OptimizedSDXL(config=config)\n",
        "\n",
        "# Generate an image\n",
        "landscape_prompt = \"A picturesque mountain range with a tranquil river flowing through it\"\n",
        "generate_landscape_image(optimized_sdxl, prompt=landscape_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hZUfE3GVE2F"
      },
      "outputs": [],
      "source": [
        "# 1. First, fix the generate_landscape_image function definition\n",
        "def generate_landscape_image(\n",
        "    model: OptimizedSDXL,\n",
        "    prompt: str,\n",
        "    negative_prompt: Optional[str] = None,\n",
        "    num_inference_steps: int = 50,\n",
        "    processor: Optional[PromptProcessor] = None  # Add processor parameter\n",
        ") -> Optional[Image.Image]:\n",
        "    \"\"\"Generate landscape image with prompt processing and memory optimization.\"\"\"\n",
        "    try:\n",
        "        # Process prompt if processor provided\n",
        "        parameters = {}\n",
        "        if processor:\n",
        "            parameters = processor.process_prompt(prompt)\n",
        "\n",
        "        print(f\"Generating image for prompt: '{prompt}'...\")\n",
        "\n",
        "        # Clear memory\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Generate with parameters\n",
        "        image = model.generate_image(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            **parameters\n",
        "        )\n",
        "\n",
        "        return image\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2DUYxIoVE2G"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize processor and model\n",
        "    processor = PromptProcessor()\n",
        "    config = OptimizationConfig(image_size=(512, 512))\n",
        "    model = OptimizedSDXL(config=config)\n",
        "\n",
        "    # Generate image with processed prompt\n",
        "    prompt = \"A large mountain landscape with dramatic sunset lighting\"\n",
        "    image = generate_landscape_image(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        negative_prompt=\"blur, distortion, low quality\",\n",
        "        num_inference_steps=50,\n",
        "        processor=processor  # Pass processor as a named argument\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09b815fe79a346068c7430a2f6f77657": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53aa0965fdb6414e9085c5d4b84c2b9f",
              "IPY_MODEL_c6719c8309d149f6909ae09ef1369868",
              "IPY_MODEL_893d98e8b2264fe287bf4da582897c0a"
            ],
            "layout": "IPY_MODEL_249aca2ae7c94eb687e2e31e17219f57"
          }
        },
        "1b41051e04e244119a79399022c1bc03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "249aca2ae7c94eb687e2e31e17219f57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53aa0965fdb6414e9085c5d4b84c2b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c11832035b7d41149754f0c50cfb7fdd",
            "placeholder": "​",
            "style": "IPY_MODEL_dbc488b7e34445cca196937d7234e27a",
            "value": ""
          }
        },
        "597974790f1e40c381046005e499f80b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "893d98e8b2264fe287bf4da582897c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4fc1c390fe141b68c89c3748adda8f3",
            "placeholder": "​",
            "style": "IPY_MODEL_a578cb57bab247c3acf77851ec28e866",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "a578cb57bab247c3acf77851ec28e866": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4fc1c390fe141b68c89c3748adda8f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c11832035b7d41149754f0c50cfb7fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6719c8309d149f6909ae09ef1369868": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_597974790f1e40c381046005e499f80b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b41051e04e244119a79399022c1bc03",
            "value": 0
          }
        },
        "dbc488b7e34445cca196937d7234e27a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}