{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LX0c1n03QZmE",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from typing import Optional, Union, List, Tuple, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from google.colab import drive\n",
    "from tqdm import tqdm\n",
    "\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kdd-efs9Qqd6",
    "outputId": "f993c82d-57ee-4a0c-9062-06615188ec3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Drive and setup paths\n",
    "drive.mount('/content/drive')\n",
    "ROOT_DIR = '/content/drive/MyDrive/SDXL_images'\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, 'landscape_images')\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'generated_images')\n",
    "CHECKPOINT_DIR = os.path.join(ROOT_DIR, 'checkpoints')\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [ROOT_DIR, IMAGE_DIR, OUTPUT_DIR, CHECKPOINT_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e1AOPGSfQ1eB"
   },
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class DriveImageDataset(Dataset):\n",
    "    def __init__(self, image_size: Tuple[int, int] = (512, 512), transform=None):\n",
    "        self.folder_path = Path(IMAGE_DIR)\n",
    "        self._validate_drive_folder()\n",
    "        self.image_files = self._get_drive_images()\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform or self._default_transform()\n",
    "        print(f\"Loaded dataset with {len(self.image_files)} images from {IMAGE_DIR}\")\n",
    "\n",
    "    def _validate_drive_folder(self):\n",
    "        if not self.folder_path.exists():\n",
    "            raise ValueError(f\"Drive image directory not found: {IMAGE_DIR}\")\n",
    "\n",
    "    def _get_drive_images(self) -> List[Path]:\n",
    "        files = list(self.folder_path.glob(\"*.jpg\")) + list(self.folder_path.glob(\"*.png\"))\n",
    "        if not files:\n",
    "            raise ValueError(f\"No images found in Drive folder {IMAGE_DIR}\")\n",
    "        return files\n",
    "\n",
    "    def _default_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return {\"image\": image, \"prompt\": f\"A landscape photo of {image_path.stem}\", \"path\": str(image_path)}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "@dataclass\n",
    "class OptimizationConfig:\n",
    "    image_size: Tuple[int, int] = (512, 512)\n",
    "    precision: str = \"fp16\"\n",
    "    enable_checkpointing: bool = True\n",
    "    enable_attention_slicing: bool = True\n",
    "    enable_sequential_cpu_offload: bool = False\n",
    "    vae_slicing: bool = True\n",
    "\n",
    "class OptimizedSDXL:\n",
    "    def __init__(self, model_id: str = \"stabilityai/stable-diffusion-xl-base-1.0\", config: Optional[OptimizationConfig] = None):\n",
    "        self.config = config or OptimizationConfig()\n",
    "        self.accelerator = Accelerator()\n",
    "        self.setup_pipeline(model_id)\n",
    "\n",
    "    def setup_pipeline(self, model_id: str):\n",
    "        try:\n",
    "            self.pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16 if self.config.precision == \"fp16\" else torch.bfloat16,\n",
    "                use_safetensors=True\n",
    "            )\n",
    "            if self.config.enable_attention_slicing:\n",
    "                self.pipeline.enable_attention_slicing()\n",
    "            if self.config.enable_sequential_cpu_offload:\n",
    "                self.pipeline.enable_sequential_cpu_offload()\n",
    "            if self.config.vae_slicing:\n",
    "                self.pipeline.enable_vae_slicing()\n",
    "            self.pipeline = self.pipeline.to(self.accelerator.device)\n",
    "            if self.config.enable_checkpointing:\n",
    "                self.pipeline.unet.enable_gradient_checkpointing()\n",
    "            print(f\"Pipeline setup complete on device: {self.accelerator.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up pipeline: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def generate_image(self, prompt: str, negative_prompt: Optional[str] = None, num_inference_steps: int = 50, **kwargs):\n",
    "        try:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                result = self.pipeline(prompt=prompt, negative_prompt=negative_prompt, num_inference_steps=num_inference_steps, **kwargs).images[0]\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating image: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zqUKkDMlQ_3f"
   },
   "outputs": [],
   "source": [
    "def train_model(model: OptimizedSDXL, dataset: DriveImageDataset, num_epochs: int = 5, batch_size: int = 1, save_interval: int = 100):\n",
    "    try:\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = torch.optim.AdamW(model.pipeline.unet.parameters(), lr=1e-5)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        noise_scheduler = model.pipeline.scheduler\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
    "                try:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Get image and convert to latents\n",
    "                    images = batch[\"image\"].to(model.accelerator.device)\n",
    "                    latents = model.pipeline.vae.encode(images).latent_dist.sample()\n",
    "                    latents = latents * model.pipeline.vae.config.scaling_factor\n",
    "\n",
    "                    # Add noise\n",
    "                    noise = torch.randn_like(latents)\n",
    "                    batch_size = latents.shape[0]\n",
    "                    timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (batch_size,), device=latents.device).long()\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                    # Get prompt embeddings\n",
    "                    prompt_ids = model.pipeline.tokenizer(\n",
    "                        batch[\"prompt\"],\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=model.pipeline.tokenizer.model_max_length,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\"\n",
    "                    ).input_ids.to(model.accelerator.device)\n",
    "\n",
    "                    encoder_hidden_states = model.pipeline.text_encoder(prompt_ids)[0]\n",
    "\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        # Pass noisy_latents, timesteps, and encoder_hidden_states to the UNet forward method\n",
    "                        model_pred = model.pipeline.unet(\n",
    "                            noisy_latents,\n",
    "                            timesteps,\n",
    "                            encoder_hidden_states\n",
    "                        ).sample\n",
    "\n",
    "                        # Calculate loss between predicted noise and actual noise\n",
    "                        loss = torch.nn.functional.mse_loss(model_pred, noise)\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    if batch_idx % save_interval == 0:\n",
    "                        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_e{epoch}_b{batch_idx}.pt\")\n",
    "                        torch.save(model.pipeline.unet.state_dict(), checkpoint_path)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during batch {batch_idx}: {e}\")\n",
    "\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch + 1} completed with average loss: {avg_loss:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {str(e)}\")\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b1e0fdb2e6184398968f974e42aebe09",
      "c7b6739c199842c6a896bf9700b4e657",
      "b33540842a6046eab3b826063998a2e8",
      "2b2b19f584864f0a86cdc905c7ee7066",
      "7afa44b787894c3c82e7e5535780cc20",
      "4acb8b3fb4da421e97ff45e0b1229fbd",
      "e23ba2f42389416da7893d0fe293d0aa",
      "39acc478d6b94b52a8f9de76502312f3",
      "92e1089b08044351864b6819ef486b84",
      "ab6a7c40280e4063951494732cb41107",
      "5362b0d1e3c84d03b81eadd4ea679002"
     ]
    },
    "id": "JLmwJju-RH8M",
    "outputId": "31ad5032-72c3-40bb-88c0-e67137ec3e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e0fdb2e6184398968f974e42aebe09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-a73d6495a18f>:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline setup complete on device: cuda\n",
      "Loaded dataset with 7 images from /content/drive/MyDrive/SDXL_images/landscape_images\n",
      "Dataset loaded with 7 images\n",
      "Starting epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:00<00:03,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 0: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 29%|██▊       | 2/7 [00:01<00:02,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 1: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 43%|████▎     | 3/7 [00:01<00:01,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 2: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 57%|█████▋    | 4/7 [00:01<00:01,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 3: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 71%|███████▏  | 5/7 [00:02<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 4: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 86%|████████▌ | 6/7 [00:02<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 5: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 6: Input type (float) and bias type (c10::Half) should be the same\n",
      "Epoch 1 completed with average loss: 0.0000\n",
      "Starting epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:00<00:02,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 0: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 29%|██▊       | 2/7 [00:00<00:02,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 1: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 43%|████▎     | 3/7 [00:01<00:01,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 2: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 57%|█████▋    | 4/7 [00:01<00:01,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 3: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 71%|███████▏  | 5/7 [00:02<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 4: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 86%|████████▌ | 6/7 [00:02<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 5: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 6: Input type (float) and bias type (c10::Half) should be the same\n",
      "Epoch 2 completed with average loss: 0.0000\n",
      "Starting epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:00<00:02,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 0: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 29%|██▊       | 2/7 [00:00<00:02,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 1: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 43%|████▎     | 3/7 [00:01<00:02,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 2: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 57%|█████▋    | 4/7 [00:02<00:01,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 3: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 71%|███████▏  | 5/7 [00:03<00:01,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 4: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 86%|████████▌ | 6/7 [00:04<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 5: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 6: Input type (float) and bias type (c10::Half) should be the same\n",
      "Epoch 3 completed with average loss: 0.0000\n",
      "Starting epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:00<00:04,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 0: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 29%|██▊       | 2/7 [00:01<00:02,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 1: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 43%|████▎     | 3/7 [00:01<00:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 2: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 57%|█████▋    | 4/7 [00:02<00:01,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 3: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 71%|███████▏  | 5/7 [00:02<00:01,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 4: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 86%|████████▌ | 6/7 [00:03<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 5: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 6: Input type (float) and bias type (c10::Half) should be the same\n",
      "Epoch 4 completed with average loss: 0.0000\n",
      "Starting epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:00<00:02,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 0: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 29%|██▊       | 2/7 [00:00<00:01,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 1: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 43%|████▎     | 3/7 [00:01<00:01,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 2: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 57%|█████▋    | 4/7 [00:01<00:01,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 3: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 71%|███████▏  | 5/7 [00:02<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 4: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 86%|████████▌ | 6/7 [00:02<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 5: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during batch 6: Input type (float) and bias type (c10::Half) should be the same\n",
      "Epoch 5 completed with average loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully\n"
     ]
    }
   ],
   "source": [
    "def run_training():\n",
    "    try:\n",
    "        print(\"Initializing training components...\")\n",
    "        config = OptimizationConfig(image_size=(512, 512))\n",
    "        model = OptimizedSDXL(config=config)\n",
    "        dataset = DriveImageDataset(image_size=(512, 512))\n",
    "        print(f\"Dataset loaded with {len(dataset)} images\")\n",
    "\n",
    "        train_model(model=model, dataset=dataset, num_epochs=5, batch_size=1, save_interval=100)\n",
    "        print(\"Training completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error \"Input type (float) and bias type (c10::Half) should be the same\" indicates a mismatch between the data types of the input and the model parameters. This typically happens when the model is using mixed precision (e.g., `float16` for the model parameters) but the input data is in `float32`.\n",
    "\n",
    "To fix this, you need to ensure that the input data is also in `float16` when using mixed precision. Here are the steps to fix the issue:\n",
    "\n",
    "1. Convert the input data to `float16` before passing it to the model.\n",
    "2. Ensure that all operations involving the model and input data are consistent in terms of data types.\n",
    "\n",
    "Here's the corrected `train_model` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: OptimizedSDXL, dataset: DriveImageDataset, num_epochs: int = 5, batch_size: int = 1, save_interval: int = 100):\n",
    "    try:\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = torch.optim.AdamW(model.pipeline.unet.parameters(), lr=1e-5)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        noise_scheduler = model.pipeline.scheduler\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
    "                try:\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Get image and convert to latents\n",
    "                    images = batch[\"image\"].to(model.accelerator.device, dtype=torch.float16)\n",
    "                    latents = model.pipeline.vae.encode(images).latent_dist.sample()\n",
    "                    latents = latents * model.pipeline.vae.config.scaling_factor\n",
    "                    \n",
    "                    # Add noise\n",
    "                    noise = torch.randn_like(latents, dtype=torch.float16)\n",
    "                    batch_size = latents.shape[0]\n",
    "                    timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (batch_size,), device=latents.device).long()\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                    \n",
    "                    # Get prompt embeddings\n",
    "                    prompt_ids = model.pipeline.tokenizer(\n",
    "                        batch[\"prompt\"], \n",
    "                        padding=\"max_length\",\n",
    "                        max_length=model.pipeline.tokenizer.model_max_length,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\"\n",
    "                    ).input_ids.to(model.accelerator.device)\n",
    "                    \n",
    "                    encoder_hidden_states = model.pipeline.text_encoder(prompt_ids)[0]\n",
    "\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        # Pass noisy_latents, timesteps, and encoder_hidden_states to the UNet forward method\n",
    "                        model_pred = model.pipeline.unet(\n",
    "                            noisy_latents,\n",
    "                            timesteps,\n",
    "                            encoder_hidden_states\n",
    "                        ).sample\n",
    "                        \n",
    "                        # Calculate loss between predicted noise and actual noise\n",
    "                        loss = torch.nn.functional.mse_loss(model_pred, noise)\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    if batch_idx % save_interval == 0:\n",
    "                        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_e{epoch}_b{batch_idx}.pt\")\n",
    "                        torch.save(model.pipeline.unet.state_dict(), checkpoint_path)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during batch {batch_idx}: {e}\")\n",
    "                    \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch + 1} completed with average loss: {avg_loss:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {str(e)}\")\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Key changes made:\n",
    "1. Convert `images` to `float16` before passing them to the model.\n",
    "2. Ensure `noise` is also in `float16`.\n",
    "\n",
    "This should resolve the data type mismatch error and allow the training to run seamlessly."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2b2b19f584864f0a86cdc905c7ee7066": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab6a7c40280e4063951494732cb41107",
      "placeholder": "​",
      "style": "IPY_MODEL_5362b0d1e3c84d03b81eadd4ea679002",
      "value": " 7/7 [00:56&lt;00:00, 11.02s/it]"
     }
    },
    "39acc478d6b94b52a8f9de76502312f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4acb8b3fb4da421e97ff45e0b1229fbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5362b0d1e3c84d03b81eadd4ea679002": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7afa44b787894c3c82e7e5535780cc20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92e1089b08044351864b6819ef486b84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ab6a7c40280e4063951494732cb41107": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1e0fdb2e6184398968f974e42aebe09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c7b6739c199842c6a896bf9700b4e657",
       "IPY_MODEL_b33540842a6046eab3b826063998a2e8",
       "IPY_MODEL_2b2b19f584864f0a86cdc905c7ee7066"
      ],
      "layout": "IPY_MODEL_7afa44b787894c3c82e7e5535780cc20"
     }
    },
    "b33540842a6046eab3b826063998a2e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39acc478d6b94b52a8f9de76502312f3",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_92e1089b08044351864b6819ef486b84",
      "value": 7
     }
    },
    "c7b6739c199842c6a896bf9700b4e657": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4acb8b3fb4da421e97ff45e0b1229fbd",
      "placeholder": "​",
      "style": "IPY_MODEL_e23ba2f42389416da7893d0fe293d0aa",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "e23ba2f42389416da7893d0fe293d0aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
