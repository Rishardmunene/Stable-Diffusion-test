{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPa2eqCHkjGTfppYUY728qw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishardmunene/Stable-Diffusion-test/blob/train/Test_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Dependencies"
      ],
      "metadata": {
        "id": "tEd8cMtoZPN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import json\n",
        "from typing import Optional, Union, List, Tuple, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from accelerate import Accelerator\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
      ],
      "metadata": {
        "id": "-h_kW1PUZOZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive and Setup Paths"
      ],
      "metadata": {
        "id": "GR7Lb3NeZf6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCy_vNWNYkH1"
      },
      "outputs": [],
      "source": [
        "# Mount Drive and setup paths\n",
        "drive.mount('/content/drive')\n",
        "ROOT_DIR = '/content/drive/MyDrive/SDXL_images'\n",
        "IMAGE_DIR = os.path.join(ROOT_DIR, 'landscape_images')\n",
        "OUTPUT_DIR = os.path.join(ROOT_DIR, 'generated_images')\n",
        "CHECKPOINT_DIR = os.path.join(ROOT_DIR, 'checkpoints')\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [ROOT_DIR, IMAGE_DIR, OUTPUT_DIR, CHECKPOINT_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Dataset Class"
      ],
      "metadata": {
        "id": "SxTeynm2ZqBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DriveImageDataset(Dataset):\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        batch = [item for item in batch if item is not None]\n",
        "        if not batch:\n",
        "            return None\n",
        "        return {\n",
        "            'image': torch.stack([item['image'] for item in batch]),\n",
        "            'prompt': [item['prompt'] for item in batch],\n",
        "            'path': [item['path'] for item in batch]\n",
        "        }\n",
        "\n",
        "    def __init__(self, image_size: Tuple[int, int] = (512, 512), transform=None):\n",
        "        self.folder_path = Path(IMAGE_DIR)\n",
        "        self._validate_drive_folder()\n",
        "        self.image_files = self._get_drive_images()\n",
        "        self.image_size = image_size\n",
        "        self.transform = transform or self._default_transform()\n",
        "        print(f\"Loaded dataset with {len(self.image_files)} images from {IMAGE_DIR}\")\n",
        "\n",
        "    def _validate_drive_folder(self):\n",
        "        if not self.folder_path.exists():\n",
        "            raise ValueError(f\"Drive image directory not found: {IMAGE_DIR}\")\n",
        "\n",
        "    def _get_drive_images(self) -> List[Path]:\n",
        "        files = list(self.folder_path.glob(\"*.jpg\")) + list(self.folder_path.glob(\"*.png\"))\n",
        "        if not files:\n",
        "            raise ValueError(f\"No images found in Drive folder {IMAGE_DIR}\")\n",
        "        return files\n",
        "\n",
        "    def _default_transform(self):\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(self.image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_files[idx]\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return {\"image\": image, \"prompt\": f\"A landscape photo of {image_path.stem}\", \"path\": str(image_path)}\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "WvH9_naJZmEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Optimization Config and Model Class"
      ],
      "metadata": {
        "id": "rQ1knZ0aZuaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class OptimizationConfig:\n",
        "    image_size: Tuple[int, int] = (512, 512)\n",
        "    precision: str = \"fp16\"\n",
        "    enable_checkpointing: bool = True\n",
        "    enable_attention_slicing: bool = True\n",
        "    enable_sequential_cpu_offload: bool = False\n",
        "    vae_slicing: bool = True\n",
        "\n",
        "class OptimizedSDXL:\n",
        "    def __init__(self, model_id: str = \"stabilityai/stable-diffusion-xl-base-1.0\", config: Optional[OptimizationConfig] = None, checkpoint_path: Optional[str] = None):\n",
        "        self.config = config or OptimizationConfig()\n",
        "        self.accelerator = Accelerator()\n",
        "        self.setup_pipeline(model_id)\n",
        "        if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "            print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "            self.pipeline.unet.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "    def setup_pipeline(self, model_id: str):\n",
        "        try:\n",
        "            self.pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=torch.float16 if self.config.precision == \"fp16\" else torch.bfloat16,\n",
        "                use_safetensors=True\n",
        "            )\n",
        "            if self.config.enable_attention_slicing:\n",
        "                self.pipeline.enable_attention_slicing()\n",
        "            if self.config.enable_sequential_cpu_offload:\n",
        "                self.pipeline.enable_sequential_cpu_offload()\n",
        "            if self.config.vae_slicing:\n",
        "                self.pipeline.enable_vae_slicing()\n",
        "            self.pipeline = self.pipeline.to(self.accelerator.device)\n",
        "            if self.config.enable_checkpointing:\n",
        "                self.pipeline.unet.enable_gradient_checkpointing()\n",
        "            print(f\"Pipeline setup complete on device: {self.accelerator.device}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate_image(self, prompt: str, negative_prompt: Optional[str] = None, num_inference_steps: int = 50, **kwargs):\n",
        "        try:\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                result = self.pipeline(prompt=prompt, negative_prompt=negative_prompt, num_inference_steps=num_inference_steps, **kwargs).images[0]\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating image: {str(e)}\")\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "I0VWbR9kZxyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Training Function"
      ],
      "metadata": {
        "id": "cIDyfo2wZ65i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model: OptimizedSDXL, dataset: DriveImageDataset, num_epochs: int = 5, batch_size: int = 1, save_interval: int = 100):\n",
        "    try:\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=DriveImageDataset.collate_fn)\n",
        "        optimizer = torch.optim.AdamW(model.pipeline.unet.parameters(), lr=1e-5)\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "        noise_scheduler = model.pipeline.scheduler\n",
        "\n",
        "        report = []\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
        "                if batch is None:\n",
        "                    continue\n",
        "                try:\n",
        "                    optimizer.zero_grad()\n",
        "                    images = batch[\"image\"].to(model.accelerator.device, dtype=torch.float16)\n",
        "                    latents = model.pipeline.vae.encode(images).latent_dist.sample()\n",
        "                    latents = latents * model.pipeline.vae.config.scaling_factor\n",
        "\n",
        "                    noise = torch.randn_like(latents, dtype=torch.float16)\n",
        "                    batch_size = latents.shape[0]\n",
        "                    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=latents.device).long()\n",
        "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                    prompt_ids = model.pipeline.tokenizer(batch[\"prompt\"], padding=\"max_length\", max_length=model.pipeline.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\").input_ids.to(model.accelerator.device)\n",
        "                    encoder_hidden_states = model.pipeline.text_encoder(prompt_ids)[0]\n",
        "\n",
        "                    with torch.amp.autocast('cuda'):\n",
        "                        model_pred = model.pipeline.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "                        loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float())\n",
        "\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "                    num_batches += 1\n",
        "\n",
        "                    if batch_idx % save_interval == 0:\n",
        "                        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_e{epoch}_b{batch_idx}.pt\")\n",
        "                        torch.save(model.pipeline.unet.state_dict(), checkpoint_path)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during batch {batch_idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
        "            memory_usage = {\n",
        "                \"gpu_memory_mb\": torch.cuda.memory_allocated() / 1024**2,\n",
        "                \"ram_memory_mb\": psutil.Process().memory_info().rss / 1024**2,\n",
        "            }\n",
        "            report.append({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"average_loss\": avg_loss,\n",
        "                \"gpu_memory_mb\": memory_usage[\"gpu_memory_mb\"],\n",
        "                \"ram_memory_mb\": memory_usage[\"ram_memory_mb\"]\n",
        "            })\n",
        "\n",
        "            print(f\"Epoch {epoch + 1} completed with average loss: {avg_loss:.4f}, Memory: {memory_usage}\")\n",
        "\n",
        "        report_path = os.path.join(ROOT_DIR, \"training_report.json\")\n",
        "        with open(report_path, \"w\") as f:\n",
        "            json.dump(report, f, indent=4)\n",
        "        print(f\"Training report saved at {report_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training error: {str(e)}\")\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n"
      ],
      "metadata": {
        "id": "XgVOKwZIZ765"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Training or Generate Images"
      ],
      "metadata": {
        "id": "3NbPSq9OZ_TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training():\n",
        "    try:\n",
        "        print(\"Initializing training components...\")\n",
        "        config = OptimizationConfig(image_size=(512, 512))\n",
        "        model = OptimizedSDXL(config=config)\n",
        "        dataset = DriveImageDataset(image_size=(512, 512))\n",
        "        print(f\"Dataset loaded with {len(dataset)} images\")\n",
        "\n",
        "        train_model(\n",
        "            model=model,\n",
        "            dataset=dataset,\n",
        "            num_epochs=5,\n",
        "            batch_size=1,\n",
        "            save_interval=10\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Training failed: {str(e)}\")\n",
        "\n",
        "def generate_image(prompt):\n",
        "    try:\n",
        "        config = OptimizationConfig(image_size=(512, 512))\n",
        "        model = OptimizedSDXL(config=config)\n",
        "        image = model.generate_image(prompt=prompt)\n",
        "        output_path = os.path.join(OUTPUT_DIR, f\"{prompt.replace(' ', '_')}.png\")\n",
        "        image.save(output_path)\n",
        "        print(f\"Image saved at {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Generation failed: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "B_4cPKU1aCDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}