{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1eXjGXWdCmHycjSNX9T0DkRj6h7JSEyuA",
      "authorship_tag": "ABX9TyPWvr2dunX2M4IZ7RzKoZ4L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishardmunene/Stable-Diffusion-test/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8sXC1Ug6eKz",
        "outputId": "75b7dc8f-b22d-4d16-afc5-eda702219ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision accelerate diffusers transformers PIL tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from typing import Optional, List, Tuple\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from accelerate import Accelerator\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "wj6h8L1A6kk3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class SDXLConfig:\n",
        "    model_id: str = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    vae_id: str = \"madebyollin/sdxl-vae-fp16-fix\"\n",
        "    image_size: Tuple[int, int] = (1024, 1024)\n",
        "    train_batch_size: int = 1\n",
        "    num_train_epochs: int = 5\n",
        "    gradient_accumulation_steps: int = 1\n",
        "    learning_rate: float = 1e-5\n",
        "    max_grad_norm: float = 1.0\n",
        "    adam_beta1: float = 0.9\n",
        "    adam_beta2: float = 0.999\n",
        "    adam_weight_decay: float = 1e-2\n",
        "    mixed_precision: str = \"fp16\"\n",
        "    save_interval: int = 500\n",
        "    root_dir: str = \"trained_models\"\n",
        "    seed: int = 42"
      ],
      "metadata": {
        "id": "7CwW6xyQ6mrx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir: str, config: SDXLConfig):\n",
        "        self.image_dir = Path(image_dir)\n",
        "        self.config = config\n",
        "        self.image_paths = list(self.image_dir.glob(\"*.jpg\")) + list(self.image_dir.glob(\"*.png\"))\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(config.image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return {\n",
        "            \"pixel_values\": image,\n",
        "            \"prompt\": f\"A high quality photo of {image_path.stem}\"\n",
        "        }"
      ],
      "metadata": {
        "id": "R5mHWI5f6pqX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SDXLTrainer:\n",
        "    def __init__(self, config: SDXLConfig):\n",
        "        self.config = config\n",
        "        self.accelerator = Accelerator(\n",
        "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "            mixed_precision=config.mixed_precision\n",
        "        )\n",
        "        self._setup_pipeline()\n",
        "        torch.manual_seed(config.seed)\n",
        "\n",
        "    def _setup_pipeline(self):\n",
        "        self.pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "            self.config.model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            use_safetensors=True,\n",
        "            variant=\"fp16\"\n",
        "        ).to(self.accelerator.device)\n",
        "        self.pipeline.enable_vae_slicing()\n",
        "        self.pipeline.enable_attention_slicing()\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        latents = self.pipeline.vae.encode(\n",
        "            batch[\"pixel_values\"].to(dtype=torch.float16)\n",
        "        ).latent_dist.sample()\n",
        "        latents = latents * self.pipeline.vae.config.scaling_factor\n",
        "\n",
        "        noise = torch.randn_like(latents)\n",
        "        timesteps = torch.randint(\n",
        "            0, self.pipeline.scheduler.config.num_train_timesteps,\n",
        "            (latents.shape[0],), device=latents.device\n",
        "        )\n",
        "        noisy_latents = self.pipeline.scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        prompt_embeds = self.pipeline.tokenizer(\n",
        "            batch[\"prompt\"],\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.pipeline.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids.to(self.accelerator.device)\n",
        "\n",
        "        encoder_hidden_states = self.pipeline.text_encoder(prompt_embeds)[0]\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            model_pred = self.pipeline.unet(\n",
        "                noisy_latents,\n",
        "                timesteps,\n",
        "                encoder_hidden_states\n",
        "            ).sample\n",
        "\n",
        "        loss = F.mse_loss(model_pred.float(), noise.float())\n",
        "        return loss\n",
        "\n",
        "    def train(self, train_dataset):\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.config.train_batch_size,\n",
        "            shuffle=True,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.pipeline.unet.parameters(),\n",
        "            lr=self.config.learning_rate,\n",
        "            betas=(self.config.adam_beta1, self.config.adam_beta2),\n",
        "            weight_decay=self.config.adam_weight_decay\n",
        "        )\n",
        "\n",
        "        self.pipeline.unet, optimizer, train_dataloader = self.accelerator.prepare(\n",
        "            self.pipeline.unet, optimizer, train_dataloader\n",
        "        )\n",
        "\n",
        "        global_step = 0\n",
        "        for epoch in range(self.config.num_train_epochs):\n",
        "            self.pipeline.unet.train()\n",
        "            for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "                with self.accelerator.accumulate(self.pipeline.unet):\n",
        "                    loss = self.train_step(batch)\n",
        "                    self.accelerator.backward(loss)\n",
        "\n",
        "                    if self.accelerator.sync_gradients:\n",
        "                        self.accelerator.clip_grad_norm_(\n",
        "                            self.pipeline.unet.parameters(),\n",
        "                            self.config.max_grad_norm\n",
        "                        )\n",
        "\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                if (step + 1) % 10 == 0:\n",
        "                    logger.info(f\"Epoch {epoch}, Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "                if (step + 1) % self.config.save_interval == 0:\n",
        "                    self._save_checkpoint(global_step)\n",
        "                global_step += 1\n",
        "\n",
        "    def _save_checkpoint(self, step: int):\n",
        "        save_path = os.path.join(self.config.root_dir, f\"checkpoint-{step}\")\n",
        "        self.accelerator.save(\n",
        "            self.pipeline.unet.state_dict(),\n",
        "            os.path.join(save_path, \"unet.pt\")\n",
        "        )"
      ],
      "metadata": {
        "id": "1I5hIhfH6vWp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_training_environment():\n",
        "    os.chdir('/content')\n",
        "\n",
        "    root_dir = '/content'\n",
        "    image_dir = os.path.join(root_dir, 'training_images')\n",
        "    output_dir = os.path.join(root_dir, 'generated_images')\n",
        "\n",
        "    for dir_path in [root_dir, image_dir, output_dir]:\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "    image_files = list(Path(image_dir).glob('*.jpg')) + list(Path(image_dir).glob('*.png'))\n",
        "    if not image_files:\n",
        "        logger.error(f\"No images found in {image_dir}\")\n",
        "        logger.info(\"Please add .jpg or .png images to the training_images folder\")\n",
        "        logger.info(f\"Current directory contents: {os.listdir(image_dir)}\")\n",
        "        raise ValueError(f\"No training images found in {image_dir}\")\n",
        "\n",
        "    return root_dir, image_dir, output_dir, len(image_files)"
      ],
      "metadata": {
        "id": "GgQx4eF864yN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    root_dir, image_dir, output_dir, num_images = setup_training_environment()\n",
        "    logger.info(f\"Found {num_images} training images\")\n",
        "\n",
        "    config = SDXLConfig(root_dir=root_dir)\n",
        "    trainer = SDXLTrainer(config)\n",
        "    train_dataset = ImageDataset(image_dir, config)\n",
        "\n",
        "    trainer.train(train_dataset)\n",
        "    logger.info(\"Training completed.\")"
      ],
      "metadata": {
        "id": "SxQAP9wC68so"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}